{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43dc3fc9",
   "metadata": {},
   "source": [
    "### Phase 1: Foundations & Simple Graphs (The State Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58547ab4",
   "metadata": {},
   "source": [
    "##### 1. Understand the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc78084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "# Recommended approach: Use a TypedDict for clear structure\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    user_input: str # The user's initial question\n",
    "    summary_output: str  # The LLM's response or a summary\n",
    "    tool_results: list  # A list to store tool results\n",
    "\n",
    "# Initial state for a run:\n",
    "initial_state = {\"user_input\": \"What is LangGraph?\", \"summary_output\": \"\", \"tool_results\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0b55e",
   "metadata": {},
   "source": [
    "##### 2. Learn Nodes (functions, LLM calls, tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e625f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. A pure Python function node\n",
    "def log_input_node(state: GraphState) -> dict:\n",
    "    print(f\"User asked: {state['user_input']}\")\n",
    "    # This node doesn't need to change the state, so it returns an empty dict or the original state.\n",
    "    return {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ce96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_node(state: GraphState) -> dict:\n",
    "    # Pretend an LLM or tool generated this text\n",
    "    summary = f\"LangGraph orchestrates how different nodes and tools work together.\"\n",
    "    \n",
    "    # Return a partial update to the shared state\n",
    "    return {\"summary_output\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1522401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Create the graph\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Add both nodes\n",
    "graph.add_node(\"log_input\", log_input_node)\n",
    "graph.add_node(\"summarize\", summarize_node)\n",
    "\n",
    "# Define the order\n",
    "graph.set_entry_point(\"log_input\")\n",
    "graph.add_edge(\"log_input\", \"summarize\")\n",
    "\n",
    "# Compile it\n",
    "app = graph.compile()\n",
    "\n",
    "# Run it and see state updates\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- Final State ---\")\n",
    "\n",
    "print(final_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. An LLM node (using a LangChain runnable)\n",
    "# The runnable itself is the node.\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"qwen3:0.6b\")\n",
    "# This LLM node will typically be configured to output to a specific state key later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ad23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define your state\n",
    "class GraphState(TypedDict):\n",
    "    user_input: str\n",
    "    summary_output: str\n",
    "\n",
    "# Create the LLM and a simple prompt pipeline\n",
    "llm = ChatOllama(model=\"qwen3:0.6b\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Wrap the LLM chain in a node that reads state and returns a partial state update\n",
    "def llm_node(state: GraphState) -> dict:\n",
    "    try:\n",
    "        answer = chain.invoke({\"question\": state[\"user_input\"]})\n",
    "    except Exception as e:\n",
    "        answer = f\"[LLM error: {e}]\"\n",
    "    return {\"summary_output\": answer}\n",
    "\n",
    "# Create a graph\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Add the node\n",
    "graph.add_node(\"llm_node\", llm_node)\n",
    "\n",
    "# Set edges\n",
    "graph.add_edge(START, \"llm_node\")\n",
    "graph.add_edge(\"llm_node\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "# Run it\n",
    "result = app.invoke({\"user_input\": \"What is LangGraph?\", \"summary_output\": \"\"})\n",
    "print(result.get(\"summary_output\", result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559ada7",
   "metadata": {},
   "source": [
    "### Note: Using an LLM as a node in a StateGraph\n",
    "\n",
    "- LangGraph nodes should accept the shared state and return a partial state update (a dict to merge).\n",
    "- Instead of passing the raw LLM as a node, wrap it in a function that reads `user_input` and returns `{\"summary_output\": ...}`.\n",
    "- The simple prompt + `StrOutputParser` ensures the node returns a plain string for easy merging and printing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422e8a4",
   "metadata": {},
   "source": [
    "##### 3. Learn Edges (data flow between nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cde609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Assume we have three nodes: node_a, node_b, and node_c\n",
    "workflow = StateGraph(GraphState) \n",
    "\n",
    "# Add the nodes to the graph\n",
    "workflow.add_node(\"step_1_input\", log_input_node)\n",
    "workflow.add_node(\"step_2_summary\", summarize_node)\n",
    "\n",
    "# 1. Edge from starting node to the summarizer\n",
    "workflow.add_edge(\"step_1_input\", \"step_2_summary\")\n",
    "\n",
    "# 2. Edge from the summarizer to the final output (END)\n",
    "workflow.add_edge(\"step_2_summary\", END)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"step_1_input\") \n",
    "\n",
    "# When you compile and run:\n",
    "app = workflow.compile()\n",
    "result = app.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca6ecd",
   "metadata": {},
   "source": [
    "##### 4. Graph construction basics (StateGraph, add_node, add_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1355dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "class SimpleState(TypedDict):\n",
    "    count: int\n",
    "\n",
    "def add_one(state: SimpleState):\n",
    "    return {\"count\": state[\"count\"] + 1}\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(SimpleState)\n",
    "workflow.add_node(\"A\", add_one)\n",
    "workflow.add_node(\"B\", add_one) # Can reuse the function!\n",
    "\n",
    "workflow.add_edge(\"A\", \"B\") # A -> B\n",
    "workflow.add_edge(\"B\", END) # B -> END\n",
    "\n",
    "workflow.set_entry_point(\"A\") # Start at A\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run: {count: 0} -> A makes it {count: 1} -> B makes it {count: 2}\n",
    "result = app.invoke({\"count\": 0})\n",
    "print(result) \n",
    "# Output: {'count': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055aca9",
   "metadata": {},
   "source": [
    "##### 5. Building the First Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b58586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Pure Python Node\n",
    "def take_input_node(input_dict: dict) -> dict:\n",
    "    # In a real app, this would get input from a UI, here we assume it's passed directly\n",
    "    return {\"user_input\": input_dict[\"user_input\"], \"summary_output\": \"\"}\n",
    "\n",
    "def save_output_node(state: GraphState) -> dict:\n",
    "    final_summary = state[\"summary_output\"].strip()\n",
    "    print(f\"\\n--- FINAL SUMMARY ---\\n{final_summary}\")\n",
    "    # Could save to a file/DB here.\n",
    "    return {} # No state change needed, just an action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86969b",
   "metadata": {},
   "source": [
    "##### 6. Add an LLM Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the LLM and the parser\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# The function node that calls the LLM\n",
    "def summarize_node(state: GraphState) -> dict:\n",
    "    user_text = state[\"user_input\"]\n",
    "    prompt = f\"Please summarize the following text concisely:\\n\\n---\\n{user_text}\"\n",
    "\n",
    "    # Call the LLM\n",
    "    summary = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "\n",
    "    # Return the update for the state\n",
    "    return {\"summary_output\": summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3140c",
   "metadata": {},
   "source": [
    "##### 7. Connect nodes with sequential edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-using nodes from above\n",
    "\n",
    "# 1. Define the workflow structure\n",
    "workflow = StateGraph(GraphState) \n",
    "\n",
    "# 2. Add all the nodes\n",
    "workflow.add_node(\"input_handler\", take_input_node)\n",
    "workflow.add_node(\"llm_summarizer\", summarize_node)\n",
    "workflow.add_node(\"output_saver\", save_output_node)\n",
    "\n",
    "# 3. Connect them sequentially (The Assembly Line)\n",
    "workflow.add_edge(\"input_handler\", \"llm_summarizer\")\n",
    "workflow.add_edge(\"llm_summarizer\", \"output_saver\")\n",
    "workflow.add_edge(\"output_saver\", END)\n",
    "\n",
    "# 4. Set the start and compile\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce77df",
   "metadata": {},
   "source": [
    "##### 8. Pass variables in the state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5083e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# State update from take_input_node:\n",
    "# returns {\"user_input\": \"text here\"}\n",
    "\n",
    "# summarize_node reads it:\n",
    "# user_text = state[\"user_input\"] \n",
    "\n",
    "# summarize_node updates it:\n",
    "# returns {\"summary_output\": \"The summary...\"}\n",
    "\n",
    "# save_output_node reads it:\n",
    "# final_summary = state[\"summary_output\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce2c4418",
   "metadata": {},
   "source": [
    "# three phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dbc8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Langgraph\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§© Phase 1 â€“ Define the State\n",
    "\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define what data your graph will carry around\n",
    "class GraphState(TypedDict):\n",
    "    user_input: str\n",
    "    summary_output: str\n",
    "\n",
    "# âš™ï¸ Phase 2 â€“ Create the Nodes\n",
    "\n",
    "# ðŸŸ¢ Node 1: Handle Input\n",
    "def take_input_node(state: GraphState) -> dict:\n",
    "    print(f\"ðŸŸ¢ Got input: {state['user_input']}\")\n",
    "    # No change needed yet, just confirming we received input\n",
    "    return {}\n",
    "\n",
    "# ðŸ§  Node 2: LLM Summarizer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"qwen3:0.6b\")\n",
    "\n",
    "\n",
    "def summarize_node(state: GraphState) -> dict:\n",
    "    user_text = state[\"user_input\"]\n",
    "    prompt = f\"Summarize this text briefly:\\n\\n{user_text}\"\n",
    "    summary = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    print(\"ðŸ§  LLM finished summarizing.\")\n",
    "    return {\"summary_output\": summary}\n",
    "\n",
    "# ðŸ“¦ Node 3: Output Saver\n",
    "\n",
    "def save_output_node(state: GraphState) -> dict:\n",
    "    print(\"\\n--- FINAL SUMMARY ---\")\n",
    "    print(state[\"summary_output\"])\n",
    "    return {}\n",
    "\n",
    "\n",
    "# ðŸ”— Phase 3 â€“ Build and Run the Graph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"input_handler\", take_input_node)\n",
    "workflow.add_node(\"llm_summarizer\", summarize_node)\n",
    "workflow.add_node(\"output_saver\", save_output_node)\n",
    "\n",
    "# Connect them in sequence\n",
    "workflow.add_edge(\"input_handler\", \"llm_summarizer\")\n",
    "workflow.add_edge(\"llm_summarizer\", \"output_saver\")\n",
    "workflow.add_edge(\"output_saver\", END)\n",
    "\n",
    "# Set entry point and compile\n",
    "workflow.set_entry_point(\"input_handler\")\n",
    "app = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d6c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Got input: LangGraph lets you connect Python logic and LLMs into agentic workflows.\n",
      "ðŸ§  LLM finished summarizing.\n",
      "\n",
      "--- FINAL SUMMARY ---\n",
      "LangGraph combines Python logic with LLMs to create agentic workflows.\n",
      "\n",
      "âœ… Final state returned by LangGraph:\n",
      "{'user_input': 'LangGraph lets you connect Python logic and LLMs into agentic workflows.', 'summary_output': 'LangGraph combines Python logic with LLMs to create agentic workflows.'}\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke({\"user_input\": \"LangGraph lets you connect Python logic and LLMs into agentic workflows.\"})\n",
    "print(\"\\nâœ… Final state returned by LangGraph:\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
